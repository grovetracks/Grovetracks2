# Ollama Local AI Composition Generation

## Overview

Local alternative to the Claude Sonnet API for generating AI compositions. Uses Ollama running in Docker with NVIDIA GPU passthrough, accessible over LAN from the development machine. Produces the same domain model output (normalized stroke coordinates) using the shared `AiCompositionPrompts` schema and `AiCompositionMapper`.

## Architecture

```
Dev Machine (Windows)                    GPU Machine (Linux/Windows)
+---------------------------+            +---------------------------+
| Grovetracks.Etl           |  HTTP      | Docker                    |
| generate-local-ai-comps   | ---------> | ollama/ollama             |
|                           |  :11434    | + NVIDIA GPU passthrough  |
| PostgreSQL (local)        |            | + qwen2.5:14b model       |
+---------------------------+            +---------------------------+
```

## Why Ollama

- Free (no per-call API costs) — unlimited tuning iterations
- Ollama v0.5+ enforces structured JSON output via GBNF grammar (`format` parameter with JSON schema)
- Docker image with NVIDIA Container Toolkit for GPU passthrough
- Simple REST API (`/api/chat`) compatible with `HttpClient`
- `OLLAMA_HOST=0.0.0.0` for LAN access from any machine

## Model Recommendation

**Default: `qwen2.5:14b`** (~9GB VRAM)
- Strong structured JSON output compliance with Ollama's `format` schema enforcement
- Good instruction following for numerical/coordinate tasks
- Fits on 12GB+ GPUs (RTX 3060 12GB, RTX 4070, RTX 3080)

**Alternatives by VRAM:**
| VRAM | Model | Size |
|------|-------|------|
| 8GB | `qwen2.5:7b` | ~5GB |
| 12-16GB | `qwen2.5:14b` (default) | ~9GB |
| 16-24GB | `qwen2.5:32b` | ~20GB |

Configurable via `--model` flag. Results tagged with `GenerationMethod` (e.g. `"ollama-qwen2.5:14b"`) for comparison in the Seeds page.

## Setup (GPU Machine)

### Prerequisites
- Docker with NVIDIA Container Toolkit installed
- NVIDIA GPU with 12GB+ VRAM

### Start Ollama
```bash
cd Grovetracks2/ollama
docker compose -f docker-compose.ollama.yml up -d
```

### Pull Model
```bash
chmod +x setup-model.sh
./setup-model.sh qwen2.5:14b
```

### Verify
```bash
curl http://localhost:11434/api/tags
```

From dev machine (replace with GPU machine IP):
```bash
curl http://192.168.1.100:11434/api/tags
```

## Usage (Dev Machine)

### Configuration

Edit `api/Grovetracks.Etl/appsettings.json`:
```json
"Ollama": {
  "Url": "http://192.168.1.100:11434",
  "DefaultModel": "qwen2.5:14b",
  "MaxRetries": 3,
  "TimeoutMinutes": 10
}
```

### CLI Commands

```powershell
cd C:\dev2\Grovetracks2\api\Grovetracks.Etl

# Check connectivity and config (no API calls)
dotnet run -- generate-local-ai-compositions --dry-run

# Generate with defaults (5 per subject, all 163 subjects)
dotnet run -- generate-local-ai-compositions

# Generate fewer per subject for testing
dotnet run -- generate-local-ai-compositions --per-subject=2

# Use a different model
dotnet run -- generate-local-ai-compositions --model=qwen2.5:32b

# Resume after interruption (skips subjects already done by this model)
dotnet run -- generate-local-ai-compositions --resume

# Clean slate
dotnet run -- generate-local-ai-compositions --truncate
```

### Flags
| Flag | Description |
|------|-------------|
| `--model=NAME` | Ollama model name (default: from config or `qwen2.5:14b`) |
| `--per-subject=N` | Compositions per subject (default: 5) |
| `--truncate` | Remove all previously AI-generated compositions |
| `--dry-run` | Check connectivity without generating |
| `--resume` | Skip subjects already generated by this specific model |

## Implementation Details

### Shared Infrastructure (Claude + Ollama)
- `AiCompositionPrompts.cs` — System prompt, JSON schema, user prompt builder
- `AiCompositionMapper.cs` — Maps AI output to `Composition` domain model (takes `generationMethod` parameter)
- `AiCompositionResponse.cs` — Shared DTOs: `AiCompositionBatch`, `AiComposition`, `AiStroke`
- `CompositionValidator.Validate()` — Quality gating

### Ollama-Specific
- `OllamaResponse.cs` — `OllamaChatResponse`, `OllamaChatMessage` DTOs
- `GenerateLocalAiCompositionsOperation.cs` — Full operation with:
  - Health check (verifies Ollama reachable and model exists)
  - Retry with temperature escalation (0.7 -> 0.8 -> 0.9)
  - Lower quality threshold (0.20 vs Claude's higher bar)
  - Graceful shutdown (`Ctrl+C` saves completed work)
  - Per-subject DB flush (no data loss on interruption)
  - Model-specific resume (tracks by `GenerationMethod`)
  - 10-minute HTTP timeout (local models are slower)

### Ollama API Request Format
```json
{
  "model": "qwen2.5:14b",
  "messages": [
    { "role": "system", "content": "<system prompt>" },
    { "role": "user", "content": "Draw 5 distinct variations of: cat..." }
  ],
  "stream": false,
  "format": { "<JSON schema>" },
  "options": { "num_predict": 4096, "temperature": 0.7 }
}
```

## Files Created/Modified

| Action | File |
|--------|------|
| New | `Etl/Models/OllamaResponse.cs` |
| New | `Etl/Operations/GenerateLocalAiCompositionsOperation.cs` |
| New | `Etl/Data/AiCompositionPrompts.cs` |
| New | `Test.Unit/Generation/OllamaResponseParsingTests.cs` (8 tests) |
| New | `ollama/docker-compose.ollama.yml` |
| New | `ollama/setup-model.sh` |
| Renamed | `Etl/Models/AiCompositionResponse.cs` (from ClaudeCompositionResponse) |
| Renamed | `Etl/Mappers/AiCompositionMapper.cs` (from ClaudeCompositionMapper) |
| Renamed | `Test.Unit/Generation/AiCompositionMapperTests.cs` (12 tests) |
| Modified | `Etl/Operations/GenerateAiCompositionsOperation.cs` (uses shared prompts) |
| Modified | `Etl/Program.cs` (new CLI command) |
| Modified | `Etl/appsettings.json` (Ollama config section) |

## Viewing Results

Results appear in the Angular Seeds page under the "AI Generated" tab, tagged with the model name (e.g. `ollama-qwen2.5:14b`). The generation method is displayed in the card metadata for easy comparison between models.
